{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f2a78b",
   "metadata": {},
   "source": [
    "# Assignment 4: Prediction of renewable energy generation\n",
    "\n",
    "## Context\n",
    "A friend recently had to sign a new electricity supply contract. The high prices surprised him very much and he decided to look into the electricity market. Among a lot of other information, he learned that electricity is also traded on an exchange.\n",
    "He found the so-called \"day-ahead\" market, where electricity is traded for the next day, the most interesting. He found out that the price is almost completely determined by the supply, because the demand hardly changes at such short notice (no private person turns on less light and no industrial company stops production at such short notice). He also found out that there are already very good forecasting models for this.\n",
    "The electricity supply, on the other hand, has become much more exciting in recent years. Wind turbines and large photovoltaic plants have hardly any running costs and can therefore undercut any other power plant (in the short term). However, their production strongly depends on the current weather in the area where the respective plant is located. The \"conventional\" power plants, which then fill the gaps to demand, determine the electricity price based on their operating costs. If you know the current oil, coal and gas prices, this is also relatively easy to predict.\n",
    "\n",
    "Meanwhile, he is sure that you could make good money if you had a good forecast of how much electricity wind power and PV will deliver. Since he has heard that you now have some experience with data analysis, he asks you to help him and to create a forecast model that predicts the amount of electricity produced (wind & photovoltaic) based on a weather report.\n",
    "\n",
    "As he is aware that data is needed for this, he has already obtained data:\n",
    " - From \"SMARTD\" (part of the regulatory authority) the installed production capacity of the different types of power plants (\"energy_installed_capacity.csv\") and the amount of electricity produced in reality (\"energy_produced.csv\").\n",
    " - Daily records from the DWD (weather service) at many measuring stations. Two files, one with the measured values (weather.csv“), one with further information (e.g. location) of the measuring stations („weatherstations.csv“).\n",
    "Both sources (all four data sets) cover the period from 2016 to 2021 inclusive.\n",
    "\n",
    "He is also sure - if the model is good enough to be worthwhile - to be able to buy sufficiently good weather forecasts, no matter in which form they would be needed exactly. So there are no limititations how the data is groupped or preprocessed.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Develop a forecast model, evaluate it and answer the question whether it would be useful for the intended use!\n",
    "Deliver a Jupyter notebook (able to run on the server) that includes the following parts:\n",
    "- Data analysis and exploration, including preparation for the model. This includes (but not limited to):\n",
    "  - understanding the data (continous/categorial, range of values ...)\n",
    "  - unifying the time base\n",
    "  - detecting and dealing missing data point\n",
    "  - possibly necessary simplifications\n",
    "- Develop and evaluate a model for the forecast.\n",
    "- Conclude whether (at least on the basis of the data) a meaningfully usable forecasting model could be achieved. \n",
    "\n",
    "Also leave drafts steps in the Jupyter notebook so that we can understand your approach.\n",
    "For each decision that is relevant to the result, give a brief justification, if not clear from the context. So after a parameter analysis comparable to Task 3 in Assignment 3, no justification would of course be needed for the choice of epochs and learning rate. No justification is necessary in an \"exploratory phase\" either, as these form the basis for later justifications.     \n",
    "\n",
    "**Hints:**\n",
    " - Do not underestimate the importance of the data preprocessing.\n",
    " - Remember what we talked in the different lectures, where we have talked about different ways to solve different problems. For time reasons we often had just choosen one, but that one do not necesarily be the right one in this assignment.\n",
    " - The data is quite \"raw\", it has some faults and/or is not in the shape you may need it and can include unnecessary information.\n",
    " - Use your \"common sense\" especially during the preprocessing stage.\n",
    " - You can add an arbitrarily number of additional cells of course.\n",
    " - If you want to use additional python libraries, just ask, usually we will be able to provide them.\n",
    " - If you have more than one idea to solve a problem, allow yourself to experiment a bit! There is not only one solution but at the end make very clear what is your final result.\n",
    " - For EDA/Data Preparation and ANN there are total of 60 points. Roughly equal distributed, but depending where you make some decisions there can a bit movement.\n",
    " - For EDA/Data preparation you may consider, as an example, selecting the three measured parameters and compare the distribution of the values in terms of time, location, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c77e32",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis & Data preparation (~30 Points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe7bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime\n",
    "\n",
    "# keras imports for the dataset and building a neural network \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "\n",
    "#sklearn imports for preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30493a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neccessary data\n",
    "weather_df = pd.read_csv('weather.csv')\n",
    "weather_df_droplist= ['Minimum Temperature', 'Average Temperature', 'Maximum Temperature', 'relative humidity', 'average air preassure', 'Rain'] #drop the useless data without influnce on the energy_produced\n",
    "weather_df.drop( weather_df_droplist, inplace= True, axis = 1)\n",
    "\n",
    "ep_df = pd.read_csv('energy_produced.csv', sep= ';' )\n",
    "#only need PV and Wind\n",
    "ep_df_droplist= ['Water power[MW]','Biomass[MW]','Nuclear power[MW]','Brown coal[MW]','Coal[MW]','Natural gas[MW]','Pump storage[MW]','Other conventional[MW]', 'Other renewables[MW]'] \n",
    "ep_df.drop(ep_df_droplist, inplace= True, axis = 1)\n",
    "\n",
    "eic_df= pd.read_csv('energy_installed_capacity.csv', sep = ';')\n",
    "#only need PV and Wind \n",
    "eic_df_droplist = ['Biomass[MW]', 'Water power[MW]','Other renewables[MW]', 'Nuclear power[MW]', 'Brown coal[MW]', 'Coal[MW]','Natural gas[MW]','Pump storage[MW]','Other conventional[MW]'] \n",
    "eic_df.drop(eic_df_droplist, inplace = True, axis = 1)\n",
    "\n",
    "ws_df = pd.read_csv('weatherstations.csv')\n",
    "ws_df_droplist = ['Operator']\n",
    "ws_df.drop(ws_df_droplist, inplace = True, axis= 1)\n",
    "\n",
    "#converting date columnes from string to date type and setting date as index\n",
    "#ep_df['Date'] = pd.to_datetime(ep_df['Date'], format= '%d.%m.%Y')\n",
    "eic_df['Date'] = pd.to_datetime(eic_df['Date'], format= '%d.%m.%Y')\n",
    "weather_df['Date'] = pd.to_datetime(weather_df['Date'], format= '%Y-%m-%d')\n",
    "\n",
    "#ep_df.set_index('Date', inplace = True)\n",
    "eic_df.set_index('Date', inplace = True)\n",
    "#weather_df.set_index('Date', inplace = True)\n",
    "\n",
    "# calculating the utilization rate of the energy installed capacity over each day \n",
    "\n",
    "def preprocess_energy():\n",
    "    start_date = datetime.date(2016, 1, 1)   # setting start date for the loop, because there are more than one row per date\n",
    "    end_date = datetime.date(2021, 12, 31)     # setting end date for loop #2021, 12, 31 subset of the given \n",
    "    delta = datetime.timedelta(days=1)       # time increase loop    \n",
    "    \n",
    "    onshore_list = []                        # lists to fill with data \n",
    "    offshore_list = []\n",
    "    pv_list= []\n",
    "    date_list = []\n",
    "\n",
    "    while start_date <= end_date:           \n",
    "        filt_ep = (ep_df['Date'] == start_date.strftime('%d.%m.%Y'))   #filter each day in given dataframe\n",
    "        ep_date_mean = ep_df[filt_ep].mean(numeric_only = True)        # calculate the mean of each column for the filtered data  #resample method dataframe %day\n",
    "        offshore_list.append(round(ep_date_mean[0]/eic_df.loc[start_date.strftime('%Y'),'Wind Offshore[MW]'].values[0]*100, 2)) # dividing the mean data by the energy installed capacity\n",
    "        onshore_list.append(round(ep_date_mean[1]/eic_df.loc[start_date.strftime('%Y'),'Wind Onshore[MW]'].values[0]*100, 2))   # and storing the value as utilization rate in % in the lists\n",
    "        pv_list.append(round(ep_date_mean[2]/eic_df.loc[start_date.strftime('%Y'),'Photovoltaics[MW]'].values[0]*100, 2))\n",
    "        date_list.append(start_date)\n",
    "        start_date += delta\n",
    "\n",
    "    df = pd.DataFrame(zip(date_list, pv_list, offshore_list, onshore_list), columns= ['Date', 'PV [%]', 'Offshore [%]', 'Onshore [%]']) #building a new dataframe for further tasks\n",
    "    return df\n",
    "\n",
    "p_ep_df = preprocess_energy()\n",
    "\n",
    "# Dividing the weatherstations in groups by lattitude\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_weather():\n",
    "    \n",
    "    lat_list = [47,48,49,50,51,52,53,54]  #list of all lattitudes in germany\n",
    "    list = []\n",
    "    df_list = []\n",
    "    ws_df['geographic latitude'] = ws_df['geographic latitude'].astype(int) #convert type from float to int to filter\n",
    "    \n",
    "    for i in lat_list:\n",
    "        filt_df = ws_df['geographic latitude'] == i         # filter only stations with the lattitude i \n",
    "        df = ws_df[filt_df]                                 # apply filter to df \n",
    "        list = df['Stations_ID'].tolist()                   # filter station ids for the lattitude i \n",
    "        filt_df = weather_df['Stations_ID'].isin(list)      # create filter for station id to df \n",
    "        st_weather_df = weather_df[filt_df]                            # apply filter to df and save \n",
    "        \n",
    "        start_date = datetime.date(2016, 1, 1)   # setting start date for the loop, because there are more than one row per date\n",
    "        end_date = datetime.date(2021, 12, 31)     # setting end date for loop #2021, 12, 31 subset of the given \n",
    "        delta = datetime.timedelta(days=1)       # time increase loop    \n",
    "    \n",
    "        avg_wind_list = []\n",
    "        max_wind_list = []\n",
    "        sun_list= []\n",
    "        cloud_list = []\n",
    "        date_list = []\n",
    "\n",
    "\n",
    "        while start_date <= end_date:\n",
    "            filt_weather = (st_weather_df['Date'] == start_date.strftime('%d.%m.%Y'))        #filter each day in given dataframe\n",
    "            weather_date_mean = st_weather_df[filt_weather].mean(numeric_only = True)        # calculate the mean of each column for the filtered data. 0 is != NaN\n",
    "            avg_wind_list.append(round(weather_date_mean[1], 2))\n",
    "            max_wind_list.append(round(weather_date_mean[2], 2))   \n",
    "            sun_list.append(round(weather_date_mean[3], 2))\n",
    "            cloud_list.append(round(weather_date_mean[4], 2))\n",
    "            date_list.append(start_date)\n",
    "            start_date += delta\n",
    "            df = pd.DataFrame(zip(date_list, avg_wind_list, max_wind_list, sun_list, cloud_list), columns= ['Date', 'Avg. Windspeed', 'Max. Windspeed', 'Sunshine duration', 'Cloud'])\n",
    "            df = df.set_index('Date')\n",
    "        \n",
    "        \n",
    "        df_list.append(df)\n",
    "\n",
    "    return df_list\n",
    "\n",
    "test_list=preprocess_weather()\n",
    "\n",
    "p_weather_df = pd.concat(test_list, axis = 1) # Merging the two dataframes together by date as index # def preprocess_weather():\n",
    "\n",
    "#powerweather_df = p_weather_df.set_index('Date').join(p_ep_df.set_index('Date')) # Merging the two dataframes together by date as index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5381c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1644, 32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = preprocessing.normalize(p_weather_df)\n",
    "scaled_weather_df = pd.DataFrame(n)\n",
    "\n",
    "n = preprocessing.normalize(p_ep_df.set_index('Date'))\n",
    "scaled_ep_df = pd.DataFrame(n)\n",
    "\n",
    "weather_train, weather_test, ep_train, ep_test = train_test_split(scaled_weather_df , scaled_ep_df, test_size = 0.25)\n",
    "\n",
    "weather_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5e819",
   "metadata": {},
   "source": [
    "## Developing and evaluation of the ANN (~30 Points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1688597",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(32,)))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics= 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "776cbccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/52 [========>.....................] - ETA: 0s - loss: 1.5668 - accuracy: 0.5987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 14:32:36.997449: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 1s 11ms/step - loss: 1.5406 - accuracy: 0.7049 - val_loss: 1.5045 - val_accuracy: 0.7993\n",
      "Epoch 2/10\n",
      " 9/52 [====>.........................] - ETA: 0s - loss: 1.5695 - accuracy: 0.7382"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-04 14:32:37.484757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 9ms/step - loss: 1.5438 - accuracy: 0.7803 - val_loss: 1.5116 - val_accuracy: 0.7993\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 1.5560 - accuracy: 0.7919 - val_loss: 1.5115 - val_accuracy: 0.7993\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5541 - accuracy: 0.7842 - val_loss: 1.5088 - val_accuracy: 0.7993\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5614 - accuracy: 0.7857 - val_loss: 1.5446 - val_accuracy: 0.7993\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5648 - accuracy: 0.7928 - val_loss: 1.5504 - val_accuracy: 0.7993\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5671 - accuracy: 0.7837 - val_loss: 1.5351 - val_accuracy: 0.7993\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5656 - accuracy: 0.7809 - val_loss: 1.5335 - val_accuracy: 0.7993\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5720 - accuracy: 0.7813 - val_loss: 1.4846 - val_accuracy: 0.7993\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.5552 - accuracy: 0.7780 - val_loss: 1.5385 - val_accuracy: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16e3dbca0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(weather_train, ep_train, epochs = 10, verbose=True, validation_data=(weather_test, ep_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06636f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9948b1c8",
   "metadata": {},
   "source": [
    "## Summary (10 Points)\n",
    "\n",
    "Is this model usable for predicting the amount of generated renewable energy based on weather data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc38b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ba8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
